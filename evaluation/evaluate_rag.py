import os
import json
from langchain_groq import ChatGroq
from dotenv import load_dotenv
from rag_core.query_logic import ask_question

load_dotenv()

judge_llm = ChatGroq(
    model_name="llama-3.1-8b-instant",
    temperature=0,
    groq_api_key=os.getenv("GROQ_API_KEY")
)

eval_data = [
    {
        "question": "‡§ò‡§£‡•ç‡§ü‡§æ ‡§µ‡§®‡•á ‡§ï‡§•‡§Æ‡•ç ‡§Ö‡§™‡§§‡§§‡•ç ?",
        "ground_truth": "‡§ö‡•ã‡§∞‡§É ‡§µ‡•ç‡§Ø‡§æ‡§ò‡•ç‡§∞‡•á‡§£ ‡§π‡§§‡§É ‡§§‡§¶‡§æ ‡§ò‡§£‡•ç‡§ü‡§æ ‡§µ‡§®‡•á ‡§Ö‡§™‡§§‡§§‡•ç"
    },
    {
        "question": "‡§ö‡•ã‡§∞‡§É ‡§ï‡§•‡§Ç ‡§Æ‡•É‡§§‡§É ?",
        "ground_truth": "‡§ö‡•ã‡§∞‡§É ‡§µ‡•ç‡§Ø‡§æ‡§ò‡•ç‡§∞‡•á‡§£ ‡§π‡§§‡§É"
    },
    {
        "question": "‡§µ‡§æ‡§®‡§∞‡§æ‡§É ‡§ï‡§ø‡§Ç ‡§Ö‡§ï‡•Å‡§∞‡•ç‡§µ‡§®‡•ç ?",
        "ground_truth": "‡§µ‡§æ‡§®‡§∞‡§æ‡§É ‡§ò‡§£‡•ç‡§ü‡§æ‡§Ç ‡§π‡§∏‡•ç‡§§‡•á ‡§ß‡•É‡§§‡•ç‡§µ‡§æ ‡§Ö‡§ß‡•Å‡§®‡•ç‡§µ‡§®‡•ç"
    },
    {
        "question": "‡§ò‡§£‡•ç‡§ü‡§æ‡§®‡§æ‡§¶‡§É ‡§ï‡§•‡§Æ‡•ç ‡§Ö‡§ú‡§æ‡§Ø‡§§‡•ç ?",
        "ground_truth": "‡§µ‡§æ‡§®‡§∞‡•à‡§É ‡§ò‡§£‡•ç‡§ü‡§æ ‡§Ö‡§ß‡•Å‡§®‡•ç‡§Ø‡§Æ‡§æ‡§®‡§æ ‡§ò‡§£‡•ç‡§ü‡§æ‡§®‡§æ‡§¶‡§É ‡§Ö‡§ú‡§æ‡§Ø‡§§‡•ç"
    },
    {
        "question": "‡§ú‡§®‡§æ‡§É ‡§ï‡§ø‡§Ç ‡§Ö‡§∂‡§ô‡•ç‡§ï‡§®‡•ç‡§§ ?",
        "ground_truth": "‡§ú‡§®‡§æ‡§É ‡§Ö‡§∂‡§ô‡•ç‡§ï‡§®‡•ç‡§§ ‡§Ø‡§§‡•ç ‡§∂‡§ø‡§ñ‡§∞‡§™‡•ç‡§∞‡§¶‡•á‡§∂‡•á ‡§ò‡§£‡•ç‡§ü‡§æ‡§ï‡§∞‡•ç‡§£‡§É ‡§®‡§æ‡§Æ ‡§∞‡§æ‡§ï‡•ç‡§∑‡§∏‡§É ‡§µ‡§∞‡•ç‡§§‡§§‡•á"
    },
    {
        "question": "‡§∞‡§æ‡§ú‡§æ ‡§ï‡§ø‡§Æ‡•ç ‡§Ö‡§ò‡•ã‡§∑‡§Ø‡§§‡•ç ?",
        "ground_truth": "‡§Ø‡§É ‡§ò‡§£‡•ç‡§ü‡§æ‡§ï‡§∞‡•ç‡§£‡§Ç ‡§®‡§æ‡§∂‡§Ø‡•á‡§§‡•ç ‡§§‡§∏‡•ç‡§Æ‡•à ‡§∞‡§æ‡§ú‡§æ ‡§∏‡•Å‡§µ‡§∞‡•ç‡§£‡§Ç ‡§¶‡§æ‡§∏‡•ç‡§Ø‡§§‡§ø ‡§á‡§§‡§ø ‡§Ö‡§ò‡•ã‡§∑‡§Ø‡§§‡•ç"
    },
    {
        "question": "‡§ò‡§£‡•ç‡§ü‡§æ‡§ï‡§∞‡•ç‡§£‡§É ‡§ï‡•Å‡§§‡•ç‡§∞ ‡§µ‡§∏‡§§‡§ø ‡§∏‡•ç‡§Æ ?",
        "ground_truth": "‡§ò‡§£‡•ç‡§ü‡§æ‡§ï‡§∞‡•ç‡§£‡§É ‡§™‡§∞‡•ç‡§µ‡§§‡§∏‡•ç‡§Ø ‡§∂‡§ø‡§ñ‡§∞‡§™‡•ç‡§∞‡§¶‡•á‡§∂‡•á ‡§µ‡§∏‡§§‡§ø ‡§∏‡•ç‡§Æ"
    },
    {
        "question": "‡§ï‡§É ‡§ò‡§£‡•ç‡§ü‡§æ‡§ï‡§∞‡•ç‡§£‡§∏‡•ç‡§Ø ‡§∞‡§π‡§∏‡•ç‡§Ø‡§Ç ‡§ú‡•ç‡§û‡§æ‡§§‡§µ‡§æ‡§®‡•ç ?",
        "ground_truth": "‡§è‡§ï‡§æ ‡§µ‡•É‡§¶‡•ç‡§ß‡§æ ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ò‡§£‡•ç‡§ü‡§æ‡§ï‡§∞‡•ç‡§£‡§∏‡•ç‡§Ø ‡§∞‡§π‡§∏‡•ç‡§Ø‡§Ç ‡§ú‡•ç‡§û‡§æ‡§§‡§µ‡§§‡•Ä"
    },
    {
        "question": "‡§µ‡•É‡§¶‡•ç‡§ß‡§æ ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ï‡§ø‡§Ç ‡§¶‡•É‡§∑‡•ç‡§ü‡§µ‡§§‡•Ä ?",
        "ground_truth": "‡§∏‡§æ ‡§¶‡•É‡§∑‡•ç‡§ü‡§µ‡§§‡•Ä ‡§Ø‡§§‡•ç ‡§µ‡§æ‡§®‡§∞‡§æ‡§É ‡§ò‡§£‡•ç‡§ü‡§æ‡§Ç ‡§Ö‡§ß‡•Å‡§®‡•ç‡§µ‡§®‡•ç‡§§‡§ø"
    },
    {
        "question": "‡§µ‡•É‡§¶‡•ç‡§ß‡§æ ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§∞‡§æ‡§ú‡§æ‡§®‡§Ç ‡§ï‡§ø‡§Æ‡•ç ‡§Ö‡§µ‡§¶‡§§‡•ç ?",
        "ground_truth": "‡§µ‡•É‡§¶‡•ç‡§ß‡§æ ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§∞‡§æ‡§ú‡§æ‡§®‡§Ç ‡§Ö‡§µ‡§¶‡§§‡•ç ‡§Ø‡§§‡•ç ‡§ò‡§£‡•ç‡§ü‡§æ‡§ï‡§∞‡•ç‡§£‡§É ‡§®‡§æ‡§∏‡•ç‡§§‡§ø ‡§ï‡•á‡§µ‡§≤‡§Ç ‡§µ‡§æ‡§®‡§∞‡§æ‡§É ‡§ò‡§£‡•ç‡§ü‡§æ‡§Ç ‡§µ‡§æ‡§¶‡§Ø‡§®‡•ç‡§§‡§ø"
    }
]


def judge_answer(question, answer, ground_truth):
    prompt = f"""
You are evaluating a Sanskrit Question Answering system.

QUESTION: {question}

GROUND TRUTH ANSWER:
{ground_truth}

MODEL ANSWER:
{answer}

Judge the model answer on:

1. Is the answer factually correct compared to ground truth?
2. Does the answer stay grounded in the story context?
3. Does it add hallucinated or unrelated information?

Respond ONLY in this format:

Correctness: 0 or 1  
Grounded: 0 or 1  
Hallucination: Yes or No
"""
    response = judge_llm.invoke(prompt).content.strip()
    return response


def parse_judgment(text):
    lines = text.splitlines()
    result = {"correctness": 0, "grounded": 0, "hallucination": "Yes"}

    for line in lines:
        if "Correctness" in line:
            result["correctness"] = int(line.split(":")[1].strip())
        elif "Grounded" in line:
            result["grounded"] = int(line.split(":")[1].strip())
        elif "Hallucination" in line:
            result["hallucination"] = line.split(":")[1].strip()

    return result


print("\nüß™ LLM JUDGE EVALUATION\n")

results = []

correct_total = 0
grounded_total = 0
hallucination_total = 0

for item in eval_data:
    result = ask_question(item["question"])
    answer = result["answer"]

    judgment_text = judge_answer(item["question"], answer, item["ground_truth"])
    judgment = parse_judgment(judgment_text)

    results.append({
        "question": item["question"],
        "ground_truth": item["ground_truth"],
        "model_answer": answer,
        "judgment": judgment
    })

    correct_total += judgment["correctness"]
    grounded_total += judgment["grounded"]
    hallucination_total += (1 if judgment["hallucination"] == "Yes" else 0)

    print(f"\nQ: {item['question']}")
    print(f"Model Answer: {answer}")
    print("Judge Result:", judgment)

# üìä Overall Metrics
n = len(eval_data)
metrics = {
    "correctness_accuracy": round(correct_total / n, 2),
    "grounded_rate": round(grounded_total / n, 2),
    "hallucination_rate": round(hallucination_total / n, 2)
}

print("\nüìä FINAL METRICS")
print(metrics)

# üíæ Save JSON report
output = {
    "individual_results": results,
    "overall_metrics": metrics
}

with open("evaluation_results.json", "w", encoding="utf-8") as f:
    json.dump(output, f, ensure_ascii=False, indent=4)

print("\n‚úÖ Results saved to evaluation_results.json")
